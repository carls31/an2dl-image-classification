{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11dXTRbikIJCdGAvNHvqZfzemLv_x7JFx","timestamp":1699041022104}],"collapsed_sections":["ejRCUc_eH2Hu","rXnAetzV5zXZ","rr-aG_GI5rVf","7xQ-UTCapggj","E2zeyKEWIOjZ","2ILdLRrh6RZ2","88-7yROf58_n"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Preliminaries"],"metadata":{"id":"ejRCUc_eH2Hu"}},{"cell_type":"markdown","source":["### Connect to Drive"],"metadata":{"id":"rXnAetzV5zXZ"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/AN2DL/Homework1"],"metadata":{"id":"X5BGHAeGBVRt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700339581061,"user_tz":-60,"elapsed":36433,"user":{"displayName":"Lorenzo Carlassara","userId":"00348287160673215752"}},"outputId":"464d75c9-b0ca-4c70-d349-1e35ac9c5218"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n","/gdrive/My Drive/AN2DL/Homework1\n"]}]},{"cell_type":"markdown","source":["### Import Libraries"],"metadata":{"id":"rr-aG_GI5rVf"}},{"cell_type":"code","source":["# Fix randomness and hide warnings\n","seed = 42\n","\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","import numpy as np\n","np.random.seed(seed)\n","\n","import logging\n","\n","import random\n","random.seed(seed)\n","\n","# Import tensorflow\n","import tensorflow as tf\n","from tensorflow import keras as tfk\n","from tensorflow.keras import layers as tfkl\n","tf.autograph.set_verbosity(0)\n","tf.get_logger().setLevel(logging.ERROR)\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)\n","print(\"TensorFlow version: \", tf.__version__)\n","\n","from tensorflow.keras import mixed_precision\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","# Import other libraries\n","import cv2\n","import hashlib\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","import seaborn as sns\n","from sklearn.utils import class_weight\n","from sklearn.model_selection import train_test_split\n","from skimage.metrics import structural_similarity as ssim"],"metadata":{"id":"az-VGygOBcao","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700339588172,"user_tz":-60,"elapsed":7124,"user":{"displayName":"Lorenzo Carlassara","userId":"00348287160673215752"}},"outputId":"2cf202ef-e7ec-402c-fc4e-90451320482f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow version:  2.14.0\n"]}]},{"cell_type":"markdown","source":["##### TPU connection"],"metadata":{"id":"2xz0-EDPMiNq"}},{"cell_type":"code","source":["try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n","    print(\"Device:\", tpu.master())\n","    strategy = tf.distribute.TPUStrategy(tpu)\n","\n","    policy = mixed_precision.Policy('mixed_bfloat16')\n","\n","except ValueError:\n","    print(\"Not connected to a TPU runtime. Using CPU/GPU strategy\")\n","    strategy = tf.distribute.MirroredStrategy()\n","\n","    policy = mixed_precision.Policy('mixed_float16')\n","\n","    # !pip install keras-cv\n","    # import keras_cv\n","\n","# Import the mixed precision according to the strategy\n","mixed_precision.set_global_policy(policy)\n","\n","print('Compute dtype: %s' % policy.compute_dtype)\n","print('Variable dtype: %s' % policy.variable_dtype)"],"metadata":{"id":"eaI28MWaJubh","executionInfo":{"status":"ok","timestamp":1700339590501,"user_tz":-60,"elapsed":2336,"user":{"displayName":"Lorenzo Carlassara","userId":"00348287160673215752"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9bcbbfa3-7276-4bb4-e767-fe2467ebac26"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Not connected to a TPU runtime. Using CPU/GPU strategy\n","Compute dtype: float16\n","Variable dtype: float32\n"]}]},{"cell_type":"markdown","source":["##### Define base paths"],"metadata":{"id":"IYPV6KvxID_N"}},{"cell_type":"code","source":["base_path = 'networks/'\n","submission_file ='ToySubmission/'"],"metadata":{"id":"lKmSqLs_IGQ8","executionInfo":{"status":"ok","timestamp":1700339590504,"user_tz":-60,"elapsed":11,"user":{"displayName":"Lorenzo Carlassara","userId":"00348287160673215752"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["### Import data"],"metadata":{"id":"tpNLUVp755Bh"}},{"cell_type":"markdown","source":["#### Load the dataset"],"metadata":{"id":"7xQ-UTCapggj"}},{"cell_type":"code","source":["data = np.load('data/public_data.npz', allow_pickle=True)\n","\n","# Extract the RGB images and labels\n","images = data['data']  # 4-dimensional numpy array of shape (5200, 96, 96, 3)\n","labels = data['labels']  # 1-dimensional numpy array of shape (5200,)"],"metadata":{"id":"42gSsFkwo7nI","executionInfo":{"status":"ok","timestamp":1700339599759,"user_tz":-60,"elapsed":9265,"user":{"displayName":"Lorenzo Carlassara","userId":"00348287160673215752"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["#### Outliers and duplicates cleaning"],"metadata":{"id":"X55DL6SGpZNJ"}},{"cell_type":"code","source":["def calculate_hash(image):\n","    \"\"\"\n","    Calculate the hash value for an image.\n","    \"\"\"\n","    hash_object = hashlib.md5(image.tobytes())\n","    return hash_object.hexdigest()\n","\n","shreck_hash = calculate_hash(images[58])\n","trololo_hash = calculate_hash(images[338])\n","\n","\n","# Create a hash map to track unique images\n","unique_images_map = {}\n","\n","# Indices to keep track of unique images\n","unique_indices = []\n","shreck_indices=[]\n","trololo_indices=[]\n","\n","unique_counter = 0\n","shreck_counter = 0\n","trololo_counter = 0\n","# Iterate through the images\n","for i, image in enumerate(images):\n","    # Calculate the hash value for the current image\n","    image_hash = calculate_hash(image)\n","\n","    if image_hash == shreck_hash:\n","      shreck_indices.append(i)\n","      continue\n","    if image_hash == trololo_hash:\n","      trololo_indices.append(i)\n","      continue\n","\n","    # Check if the hash value is already in the hash map\n","    if image_hash not in unique_images_map:\n","        # Add the hash value to the hash map\n","        unique_images_map[image_hash] = i\n","        # Add the index to the list of unique indices\n","        unique_indices.append(i)\n","        unique_counter = unique_counter+1\n","\n","print(\"There are \"+str(len(unique_indices))+\" unique non-outliers\")\n","print(\"There are \"+str(len(shreck_indices))+\" shrecks\")\n","print(\"There are \"+str(len(trololo_indices))+\" trololos\")\n","\n","percentage_shreck = len(shreck_indices)/len(images)\n","percentage_trololo = len(trololo_indices)/len(images)\n","print(\"The percentage of shrecks in the original dataset is \"+str(percentage_shreck))\n","print(\"The percentage of trololos in the original dataset is \"+str(percentage_trololo))\n","\n","num_shrecks_to_add = 0 #round(percentage_shreck * len(unique_indices))\n","num_trololo_to_add = 0 #round(percentage_trololo * len(unique_indices))\n","\n","print(\"Adding \"+str(num_shrecks_to_add)+\" Shrecks\")\n","print(\"Adding \"+str(num_trololo_to_add)+\" Trololos\")\n","\n","shreck_indices=(shreck_indices[:num_shrecks_to_add])\n","trololo_indices=(trololo_indices[:num_trololo_to_add])\n","\n","# Create a new dataset with unique images\n","images = images[unique_indices]\n","labels = labels[unique_indices]\n","\n","print(\"The new dataset has \"+str(len(images))+\" images\")"],"metadata":{"id":"vw4kZTU_7c5V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700339600338,"user_tz":-60,"elapsed":586,"user":{"displayName":"Lorenzo Carlassara","userId":"00348287160673215752"}},"outputId":"b39b78f7-b843-44e7-c152-2c007a386f9b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 4850 unique non-outliers\n","There are 98 shrecks\n","There are 98 trololos\n","The percentage of shrecks in the original dataset is 0.018846153846153846\n","The percentage of trololos in the original dataset is 0.018846153846153846\n","Adding 0 Shrecks\n","Adding 0 Trololos\n","The new dataset has 4850 images\n"]}]},{"cell_type":"markdown","source":["#### Train, validation, test split"],"metadata":{"id":"8bkeom4rpWe7"}},{"cell_type":"code","source":["# Define the classes\n","classes = ['healthy', 'unhealthy']\n","class_to_index = {cls: idx for idx, cls in enumerate(classes)}\n","\n","# Convert string labels to integer labels\n","labels_encoded = np.array([class_to_index[label] for label in labels])\n","\n","# Create a one-hot encoding\n","labels_encoded = tf.keras.utils.to_categorical(labels_encoded, num_classes=len(classes))\n","\n","# Split data into train_val and test sets\n","X_train_val, X_test, y_train_val, y_test = train_test_split(images, labels_encoded, random_state=seed, test_size=50, stratify=np.argmax(labels_encoded,axis=1))\n","\n","# Split data into training and validation sets, maintaining class distribution\n","X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, random_state=seed, test_size=960, stratify=np.argmax(y_train_val,axis=1))\n","\n","# Print shapes of the datasets\n","print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n","print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n","print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n","\n","# Define input shape and output shape\n","input_shape = X_train.shape[1:]\n","output_shape = y_train.shape[-1]\n","\n","print(f\"Input Shape: {input_shape}, Output Shape: {output_shape}\")"],"metadata":{"id":"yHthdBRUpU_v","executionInfo":{"status":"ok","timestamp":1700339600821,"user_tz":-60,"elapsed":488,"user":{"displayName":"Lorenzo Carlassara","userId":"00348287160673215752"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1cd657c4-742d-4a19-901f-e94f285e35c6"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train shape: (3840, 96, 96, 3), y_train shape: (3840, 2)\n","X_val shape: (960, 96, 96, 3), y_val shape: (960, 2)\n","X_test shape: (50, 96, 96, 3), y_test shape: (50, 2)\n","Input Shape: (96, 96, 3), Output Shape: 2\n"]}]},{"cell_type":"markdown","source":["#### Visualize images"],"metadata":{"id":"E2zeyKEWIOjZ"}},{"cell_type":"code","source":["# num_img = 10\n","# # 3 5 6 10 14\n","# k =15\n","# Brightness = tf.keras.Sequential([\n","#   tfkl.RandomBrightness(0.5, value_range=(0,1)),\n","# ])\n","\n","# X_train_Brightness = Brightness(X_train)\n","\n","# fig, axes = plt.subplots(2, num_img, figsize=(20,4))\n","# for i in range(num_img):\n","#     ax = axes[0,i%num_img]\n","#     ax.imshow(X_train[k*num_img+i])\n","#     ax = axes[1,i%num_img]\n","#     ax.imshow(X_train_Brightness[k*num_img+i])\n","# plt.tight_layout()\n","# plt.show()"],"metadata":{"id":"ZocyDR9fiusw","executionInfo":{"status":"ok","timestamp":1700339600821,"user_tz":-60,"elapsed":15,"user":{"displayName":"Lorenzo Carlassara","userId":"00348287160673215752"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Hyperparameters"],"metadata":{"id":"hkWP1eIk6Lmk"}},{"cell_type":"code","source":["# Define batch size, number of epochs, learning rate\n","batch_size = 64\n","epochs = 100\n","tl_learning_rate = 5e-3\n","ft_learning_rate = 5e-5\n","\n","# Dropout\n","dropout_rate=0.24\n","\n","# Early Stopping\n","es_patience=15\n","\n","# Learing Rate Scheduler\n","reducing_factor=0.1\n","lr_patience=10\n","min_lr=1e-7\n","\n","# Augmentation\n","bright = 0.16\n","contrast = 0.12\n","rotation = 0.24\n","\n","# Print batch size, epochs, learning rate\n","print(f\"Batch Size: {batch_size}, Epochs: {epochs}, Learning Rate: {ft_learning_rate}\")\n","print(f\"Dropout rate: {dropout_rate} \")\n","print(f\"Early stopping patience: {es_patience}\")"],"metadata":{"id":"bAyimchcXGpT","executionInfo":{"status":"ok","timestamp":1700339600822,"user_tz":-60,"elapsed":14,"user":{"displayName":"Lorenzo Carlassara","userId":"00348287160673215752"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1d2b8b02-545d-41b6-96cf-80dd6d12fa4a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Batch Size: 64, Epochs: 100, Learning Rate: 5e-05\n","Dropout rate: 0.24 \n","Early stopping patience: 15\n"]}]},{"cell_type":"markdown","source":["### Utils"],"metadata":{"id":"2ILdLRrh6RZ2"}},{"cell_type":"code","source":["def schedule(epoch, lr):\n","  if epoch < 10:\n","    return lr\n","  else:\n","    return lr * tf.math.exp(-0.07)\n","\n","lrs = tfk.callbacks.LearningRateScheduler(schedule, verbose=0)\n","\n","tl_es = tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=es_patience, restore_best_weights=True, mode='max')\n","ft_es = tfk.callbacks.EarlyStopping(monitor='val_accuracy', patience=es_patience, restore_best_weights=True, mode='max')\n","\n","checkpoint_name = \"kerasNet\"\n","checkpoint_folder = base_path + \"checkpoint\"\n","\n","# Define two callback functions for early stopping and learnin g rate reduction\n","callbacks=[\n","    ft_es,\n","    # tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=reducing_factor, patience=lr_patience, min_lr=min_lr, mode='max'),\n","    lrs,\n","    # tfk.callbacks.ModelCheckpoint( os.path.join(checkpoint_folder, checkpoint_name+\".keras\") )\n","]\n","\n","# Print the defined callbacks\n","print(\"Fine Tuning Callbacks:\")\n","for callback in callbacks:\n","    print(callback)"],"metadata":{"id":"Pbe3yqARtD35","executionInfo":{"status":"ok","timestamp":1700339600823,"user_tz":-60,"elapsed":13,"user":{"displayName":"Lorenzo Carlassara","userId":"00348287160673215752"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c4d4b644-a9be-433d-d4ef-858cf83a7cc5"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Fine Tuning Callbacks:\n","<keras.src.callbacks.EarlyStopping object at 0x7fd0e1458760>\n","<keras.src.callbacks.LearningRateScheduler object at 0x7fd0e1459360>\n"]}]},{"cell_type":"markdown","source":["## ConvNeXt"],"metadata":{"id":"M7sZ7Ufy1n_p"}},{"cell_type":"code","source":["model_name = 'ConvNeXtS_batch128_noOutliers_AUG'"],"metadata":{"id":"i34OoEy9eQ0E","executionInfo":{"status":"ok","timestamp":1700339600823,"user_tz":-60,"elapsed":12,"user":{"displayName":"Lorenzo Carlassara","userId":"00348287160673215752"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Define the model"],"metadata":{"id":"nlj7SYzve3ji"}},{"cell_type":"markdown","source":["Initialize ConvNeXtLarge with imagenet dataset weights"],"metadata":{"id":"aolrB8lZcKlq"}},{"cell_type":"code","source":["def create_model(loss,optimizer):\n","\n","    # Augment the input\n","    augmentation = tfk.Sequential([\n","        # tfkl.RandomContrast(contrast,seed=seed),\n","        # tfkl.RandomBrightness(bright,seed=seed),\n","        tfkl.RandomRotation(rotation,seed=seed),\n","        tfkl.RandomFlip(name='RandomFlip',seed=seed),\n","        tfkl.ZeroPadding2D((2, 2), name='ZeroPadding_2x2'),\n","        tfkl.RandomCrop(input_shape[0], input_shape[1], name='RandomCrop',seed=seed)\n","    ], name='Augment')\n","\n","    supernet = tfk.applications.ConvNeXtSmall(\n","        include_top=False,\n","        weights=\"imagenet\",\n","        input_shape=input_shape,\n","        pooling=\"avg\",\n","        classes=2\n","    )\n","\n","    # Use the supernet as feature extractor, i.e. freeze all its weigths\n","    supernet.trainable = False\n","\n","    tl_model = tf.keras.Sequential([\n","        tfk.Input(input_shape, name=\"input_layer\"),\n","        augmentation,\n","        supernet,\n","        tfkl.Dropout(dropout_rate),\n","        tfkl.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(5e-4), kernel_initializer=tfk.initializers.HeUniform(seed)),\n","        tfkl.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.L1L2(5e-4), kernel_initializer=tfk.initializers.HeUniform(seed)),\n","        tfkl.Dropout(dropout_rate),\n","        tfkl.Dense(output_shape, activation='softmax', kernel_initializer=tf.keras.initializers.GlorotUniform(seed), dtype='float32', name='output_layer')\n","    ], name = \"ConvNextLarge\")\n","\n","    # Compile the model with Categorical Cross-Entropy loss and Adam optimizer\n","    tl_model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n","\n","    return tl_model, supernet\n","\n","with strategy.scope():\n","\n","  loss=tfk.losses.CategoricalCrossentropy(),\n","  tl_optimizer=tfk.optimizers.Adam(tl_learning_rate)\n","\n","  # Create a Model connecting input and output\n","  tl_model, supernet = create_model(loss,tl_optimizer)\n","\n","tl_model.summary()"],"metadata":{"id":"K3CZcAVx5fnV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700339616371,"user_tz":-60,"elapsed":15558,"user":{"displayName":"Lorenzo Carlassara","userId":"00348287160673215752"}},"outputId":"5f4c4158-43b6-4cb0-c3ea-9cc88ee2cbd6"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/convnext/convnext_small_notop.h5\n","198551472/198551472 [==============================] - 6s 0us/step\n","Model: \"ConvNextLarge\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," Augment (Sequential)        (None, 96, 96, 3)         0         \n","                                                                 \n"," convnext_small (Functional  (None, 768)               49454688  \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 768)               0         \n","                                                                 \n"," dense (Dense)               (None, 1024)              787456    \n","                                                                 \n"," dense_1 (Dense)             (None, 512)               524800    \n","                                                                 \n"," dropout_1 (Dropout)         (None, 512)               0         \n","                                                                 \n"," output_layer (Dense)        (None, 2)                 1026      \n","                                                                 \n","=================================================================\n","Total params: 50767970 (193.66 MB)\n","Trainable params: 1313282 (5.01 MB)\n","Non-trainable params: 49454688 (188.65 MB)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["\n","### Train the top"],"metadata":{"id":"pgRZ5oUFe9tc"}},{"cell_type":"code","source":["# Train the model and save its history\n","history = tl_model.fit(\n","    x = X_train,\n","    y = y_train,\n","    batch_size=batch_size,\n","    epochs=100,\n","    validation_data=(X_val,y_val),\n","    callbacks=[tl_es],\n","    #class_weight=class_weights\n",").history"],"metadata":{"id":"S6DUbl_7fC0r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8284581b-a224-4a2d-f993-08d14e84335f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","60/60 [==============================] - 39s 256ms/step - loss: 16.2291 - accuracy: 0.5979 - val_loss: 7.1147 - val_accuracy: 0.6948\n","Epoch 2/100\n","60/60 [==============================] - ETA: 0s - loss: 4.6226 - accuracy: 0.6737"]}]},{"cell_type":"code","source":["print(\"Best epoch: \",tl_es.best_epoch)\n","print(\"Test label Shape:\", y_test.shape)\n","print(\"Evaluate on test data\")\n","print(\"test loss, test acc:\", tl_model.evaluate(X_test, y_test))"],"metadata":{"id":"kOsB3jOTE-Lu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Fine Tuning"],"metadata":{"id":"nPU1ApTr8M1L"}},{"cell_type":"markdown","source":["Copy model and weights"],"metadata":{"id":"2xAc_8a6bbxs"}},{"cell_type":"code","source":["with strategy.scope():\n","    ft_model = tl_model\n","    ft_model.set_weights(tl_model.get_weights())\n","    ft_optimizer=tfk.optimizers.Adam(ft_learning_rate)\n","    ft_model.compile(loss=loss, optimizer=ft_optimizer, metrics=['accuracy'])\n","del tl_model\n","print(\"Number of layers in the supernet: \", len(ft_model.get_layer(supernet.name).layers))"],"metadata":{"id":"T7IN2w6O9WuA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unfreeze layers"],"metadata":{"id":"lQOwsIpQbhZY"}},{"cell_type":"code","source":["with strategy.scope():\n","    ft_model.get_layer(supernet.name).trainable = True\n","\n","    # We unfreeze the top layers while leaving BatchNorm layers frozen\n","    for i in reversed(range(len(ft_model.get_layer(supernet.name).layers))):\n","        layer = ft_model.get_layer(supernet.name).layers[i]\n","        if not isinstance(layer, tfkl.BatchNormalization):\n","            layer.trainable = True\n","        else:\n","            layer.trainable = False\n","        print(i, layer.name, layer.trainable)\n","\n","    ft_optimizer=tfk.optimizers.Adam(ft_learning_rate)\n","    ft_model.compile(loss=loss, optimizer=ft_optimizer, metrics=['accuracy'])\n","\n","# Display model summary\n","ft_model.summary()"],"metadata":{"id":"_nag0aubfDcy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Train"],"metadata":{"id":"TGyc6ApiBavA"}},{"cell_type":"code","source":["ft_total_epochs = tl_es.best_epoch + epochs\n","\n","history = ft_model.fit(\n","    x=X_train,\n","    y=y_train,\n","    batch_size=batch_size,\n","    epochs=ft_total_epochs,\n","    validation_data=(X_val,y_val),\n","    initial_epoch=tl_es.best_epoch,\n","    callbacks = [ft_es]\n",").history\n","\n","saved_model_filepath = base_path + model_name\n","ft_model.save(saved_model_filepath)"],"metadata":{"id":"zoAXW2HAoBqd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Plot the results"],"metadata":{"id":"M1QzZKIabsVc"}},{"cell_type":"code","source":["# Plot training and validation performance metrics\n","fig, ax = plt.subplots(2, 1, figsize=(15, 7), sharex=True)\n","\n","ax[0].set_title(model_name+\" - Training history\", fontsize=16)\n","\n","# Plot training and validation loss\n","ax[0].plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2, linestyle='--')\n","ax[0].plot(history['val_loss'], label='Validation', alpha=0.8, color='#ff7f0e', linewidth=3)\n","ax[0].axvline(ft_es.best_epoch, color='k', ls=\"-.\", alpha=0.8, label='Best epoch')\n","ax[0].set_ylabel('Categorical Crossentropy')\n","ax[0].grid(alpha=0.3)\n","\n","# Plot training and validation accuracy, highlighting the best epoch\n","ax[1].plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2, linestyle='--')\n","ax[1].plot(history['val_accuracy'], label='Validation', alpha=0.8, color='#ff7f0e', linewidth=3)\n","ax[1].axvline(ft_es.best_epoch, color='k', ls=\"-.\", alpha=0.8, label='Best epoch')\n","ax[1].set_ylabel('Accuracy')\n","ax[1].set_xlabel('Epoch')\n","ax[1].grid(alpha=0.3)\n","ax[1].legend(loc='upper center', bbox_to_anchor=(0.5, 0.1),\n","          fancybox=True, ncol=3)\n","plt.tight_layout()\n","\n","#plt.savefig(os.path.join(checkpoint_folder, checkpoint_name+\"_training_hist.pdf\"))\n","\n","plt.show()\n","\n","print(\"Test label Shape:\", y_test.shape)\n","print(\"Evaluate on test data\")\n","print(\"test loss, test acc:\", ft_model.evaluate(X_test, y_test))"],"metadata":{"id":"BEhQMqDxfDU9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### save the model"],"metadata":{"id":"EiJoV9mCFVht"}},{"cell_type":"code","source":["saved_model_filepath = base_path + model_name\n","\n","submission_model_filepath = submission_file + 'SubmissionModel'"],"metadata":{"id":"ZZwvnnMnI8_t","executionInfo":{"status":"ok","timestamp":1700327378217,"user_tz":-60,"elapsed":4,"user":{"displayName":"Lorenzo Carlassara","userId":"00348287160673215752"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# Export trained model\n","\n","ft_model.save(saved_model_filepath)"],"metadata":{"id":"W9viAR-U5c3_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ft_model.save(submission_model_filepath)"],"metadata":{"id":"N5az8Szv5gLZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Make inference"],"metadata":{"id":"88-7yROf58_n"}},{"cell_type":"code","source":["%cd  /gdrive/My Drive/AN2DL/Homework1/ToySubmission\n","from model import model as model_py\n","\n","# Predict, perform one-hot encoding and convert to a numpy array\n","predictions = tf.one_hot(model_py(os.getcwd()).predict(X_test), depth=2).numpy()\n","\n","%cd  /gdrive/My Drive/AN2DL/Homework1\n","\n","# Display the shape of the predictions\n","print(\"Predictions Shape:\", predictions.shape)\n","\n","# Compute classification metrics\n","print('Accuracy',accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1)).round(4))\n","print('F1',f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro').round(4))\n","print('Precision',precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro').round(4))\n","print('Recall',recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro').round(4))"],"metadata":{"id":"cA3YOsbZQCT7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700155963566,"user_tz":-60,"elapsed":29640,"user":{"displayName":"Ellen Poli","userId":"11013659650977034887"}},"outputId":"e6669fce-0ae6-43bb-c828-47106bca2283"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/.shortcut-targets-by-id/1NQxXnza2eqP6GiReApeY7u4BwfqSYtaf/Homework1/ToySubmission\n","/gdrive/.shortcut-targets-by-id/1NQxXnza2eqP6GiReApeY7u4BwfqSYtaf/Homework1\n","Predictions Shape: (148, 2)\n","Accuracy 0.9054\n","F1 0.8962\n","Precision 0.9138\n","Recall 0.8855\n"]}]}]}